{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woizfsrd6-h0"
   },
   "source": [
    "<p><img alt=\"Colaboratory logo\" height=\"65px\" src=\"https://upload.wikimedia.org/wikipedia/en/thumb/b/b1/Davivienda_logo.svg/1200px-Davivienda_logo.svg.png\" align=\"left\" hspace=\"10px\" width=\"20%\" vspace=\"15px\"></p>\n",
    "\n",
    "<h1 align=\"center\"> Prueba Técnica Profesional III Departamento de Datos no Estructurados  </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRuZcJJx57vw"
   },
   "source": [
    "#### **Juan Sebastián Gómez Duque**\n",
    "#### **Estadístico | Científico de datos - Facultad de Ciencias, Universidad Nacional de Colombia**\n",
    "#### **Correo electrónico: jgomezd@unal.edu.co**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEKZolaufVTP"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un vistazo al proceso permite identificar que pueden ser muy útiles algunos métodos modernos como en los que se usan los transformadores de HugginFace. El proceso hace uso de dos de estos transformadores que se encuentran entrenados en español, dada la naturaleza del conjunto de datos y con el objetivo de extraer los tópicos principales del conjunto:\n",
    "1. Se realiza una limpieza sobre el conjunto de tweets, uniendo emojis con los tweets, eliminando dígitos y quitando carácteres especiales (como @)\n",
    "2. Se emplea un Embedding pre-entrenado de HugginFace conocido como \"sentence_similarity_spanish_es\" (esto convierte en un vector numérico cada uno de los tweets)\n",
    "3. Para el uso del modelo BERTopics (el cual es nuestra principal motivación) requiere del uso de dos herramientas más: UMAP Y HBDSCAN, los cuales reducen la dimensión de los datos y generan clusters de datos, respectivamente. El proceso de extracción de tópicos en BERTopics genera un conjunto de palabras que se asocian en mayor medida a dicho tópico.\n",
    "4. Del proceso de Cluster que se realizó por BERTopics, con ayuda de HBDSCAN, se obtienen los documentos más relevantes para cada tópico (Cluster) para así disminuir la influencia de documentos que no sean tan relevantes para generar una idea general de cada uno de los tópicos.\n",
    "5. Del proceso de BERTopics se extraen las palabras más relacionadas con cada tópico y de estas se eliminan las stopwords para posteriormente presentarlas.\n",
    "6. Por medio de un summary model llamado \"bert2bert\" con base en español se generan resumenes de cada uno de los tópicos. Para este proceso se genera un único texto para cada tópico, uniendo los tweets que más lo representan. Este proceso genera un nuevo producto que en compañia de los datos que se obtuvieron en el paso anterior, ayuda a visualizar con mayor claridad el concepto general de cada uno de los tópicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se visualiza un diagrama que representa el proceso que se está realizando en el modelaje de tópicos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/dux135/Prueba-Davivienda/blob/main/Texto/Diagrama_proceso.jpg?raw=true\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoeWZ27KZLj2"
   },
   "source": [
    "## 1. Importación y lectura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importe de librerías**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el proceso de instalación de librerías e importación de las mismas para poder correr las diferentes herramientas utilizadas en el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip intsall nltk #La librería nltk se utilizará principalmente para el uso de diccionarios stopwords con el objetivo de\n",
    "                  #depurar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers #La librería transformers es útil para importar todos los modelos y herrmiaentas necesarias\n",
    "                          #que se usarán de la plataforma HugginFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers #Librería necesaria pora el mannejo de transformadores, en particular se usará para\n",
    "                                   #realizar el embedding inicial de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33690,
     "status": "ok",
     "timestamp": 1659801280316,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "5lzeK4fyrIJt",
    "outputId": "2e408c4b-fa62-4bdc-c788-b8de069fc4a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.56.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.5.7)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (4.12.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.39.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn #Se instala la librería UMAP\n",
    "import umap #La librería UMAP brinda una metodología útil para la representación de los datos y reducir la dimensionalidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2970,
     "status": "ok",
     "timestamp": 1659801353364,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "D-V511t6tqla",
    "outputId": "5a24f2d0-5b06-4fa6-b026-8b1802b2b0ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: hdbscan in /usr/local/lib/python3.7/dist-packages (0.8.28)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.21.6)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (0.29.32)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan #Se instala la librería hdbscan\n",
    "import hdbscan #La librería hbdscan permite la creación de clusters y con esto se generan la diferencia entre documentos\n",
    "               #por tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el proceso de modelado de tópicos BERTopics se utiliza CUDA de Nvidia por lo tanto es necesario la instalación de los controladores y la versión adecuada de Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8139,
     "status": "ok",
     "timestamp": 1659805982810,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "YDxcKt_Y5ilp",
    "outputId": "31da22ce-7f9b-4b97-c1a5-6d1e2f6499bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Requirement already satisfied: torch==1.11.0+cu113 in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision==0.12.0+cu113 in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: torchaudio==0.11.0+cu113 in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0+cu113) (4.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1659801353372,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "R5SMyP7Fujeu"
   },
   "outputs": [],
   "source": [
    "!pip install bertopic #Se instala BERTopics para el modelado de tópicos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1659805993900,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "8Vm4EyuGKV48"
   },
   "outputs": [],
   "source": [
    "import pandas as pd #Se importa la librería pandas para el manejo de conjuntos de datos\n",
    "import numpy as np #Se importa numpy para el manejo de datos y operaciones matemáticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1659805993904,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "_Tgt67MiKhAb"
   },
   "outputs": [],
   "source": [
    "import os #Se importa OS, librería útil para el manejo de ubicaciones y extraer los datos de las carpetas respectivas de\n",
    "          #Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1659801241603,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "554zWg7y_zS-"
   },
   "outputs": [],
   "source": [
    "import copy #Se importa copy, librería relevante para la generación de copias en conjuntos de datos, de esta manera\n",
    "            #se conserva la integridad de los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4iY9bT1a2Ay"
   },
   "outputs": [],
   "source": [
    "import nltk #Se importa nltk, librería útil para el preprocesamiento de lenguaje natural\n",
    "nltk.download('stopwords') #Se descarga un conjunto de stopwords estandar de nltk\n",
    "from nltk.corpus import stopwords #Se importa la función stopwords que se utilizará para obtener la lista de stopwords\n",
    "                                  #en español que necesita para el manejo de tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #Es importante importar la librería torch, de la cual hará uso más adelante BERTopics \n",
    "from transformers import BertTokenizerFast, EncoderDecoderModel #Funciones utilizadas posteriormente para la tokenización e\n",
    "                                                                #implementación del modelo summary bert2bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2352,
     "status": "ok",
     "timestamp": 1659801355681,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "NpKqlsBLxJSG"
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic #Se importa la librería BERTopics para el modelado de tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3377,
     "status": "ok",
     "timestamp": 1659801283585,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "lyuimzfKpcQY"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer #Se importa librería para realizar el embedding sobre los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #Librería útil pora el manejo de expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importación de archivos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se establece la carpeta raíz donde se encuentra el conjunto de datos de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1659805996253,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "VMthpVwIKvt7"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/MyDrive/Davivienda/Prueba_conocimientos_davivienda/Ejercicio_2_Chats/Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1659805997230,
     "user": {
      "displayName": "Juan Sebastián Gómez Duque",
      "userId": "16030435857127249246"
     },
     "user_tz": 300
    },
    "id": "Ph3zBWHXKijW"
   },
   "outputs": [],
   "source": [
    "tweets=pd.read_csv(\"davivienda_tweets.csv\") #Se importa el conjunto de datos en un DataFrame de pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVhdER-pZQJL"
   },
   "source": [
    "## 2. Prepocesamiento de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen algunas funciones que serán de mucha utilidad para el proceso de procesamiento del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por medio de la función stopwords de NLTK fue posible obtener un diccionario de stopwords, que será de mucha utilidad adelante para poder eliminar paalabras que no son el foco central del texto sino más bien herramientas del lenguaje común en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOS4OyLT22ED",
    "outputId": "93fe93ea-8166-41f2-ee9e-4375954c9d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nuestros', 'soy', 'estabais', 'estar', 'esto', 'hayan', 'estando', 'quien', 'estaría', 'tendrán', 'estés', 'has', 'era', 'algo', 'ti', 'estaréis', 'tenía', 'hubiesen', 'estuviese', 'seamos', 'tendré', 'ni', 'seríais', 'davivienda', 'somos', 'habrá', 'fuese', 'estarían', 'estadas', 'también', 'del', 'han', 'estas', 'mi', 'cual', 'e', 'otros', 'ella', 'otro', 'sentido', 'habíamos', 'estaré', 'yo', 'habiendo', 'tenga', 'ante', 'vuestro', 'tanto', 'tuve', 'tengáis', 'estado', 'ese', 'hubieseis', 'de', 'hubieses', 'tuvimos', 'durante', 'tuviese', 'estuvo', 'fuimos', 'que', 'estuviéramos', 'tenías', 'habéis', 'vosotros', 'nosotros', 'donde', 'tuvieron', 'tengan', 'tuviera', 'este', 'desde', 'lo', 'fueras', 'estuvieron', 'estuviésemos', 'pero', 'fue', 'habidos', 'hubieras', 'estaremos', 'vuestra', 'tendremos', 'hubieron', 'como', 'sería', 'eras', 'fueseis', 'sentidas', 'fuerais', 'tuyos', 'suya', 'cuando', 'hasta', 'serán', 'seríamos', 'tus', 'sois', 'estos', 'hubiese', 'vuestras', 'habrías', 'habida', 'no', 'estuviste', 'habríais', 'tenidos', 'seréis', 'estuvieseis', 'mías', 'estuvierais', 'por', 'habido', 'tendréis', 'poco', 'sentid', 'tuvo', 'tuviesen', 'esos', 'nada', 'serían', 'hubieran', 'ya', 'eran', 'tu', 'nos', 'con', 'tuyas', 'estén', 'estuvieses', 'tendría', 'habíais', 'habían', 'fuisteis', 'estabas', 'un', 'seré', 'éramos', 'fui', 'tuvieseis', 'mío', 'habrán', 'había', 'le', 'una', 'tenido', 'estarás', 'hubiste', 'tuvierais', 'hubisteis', 'estamos', 'seas', 'tendrían', 'unos', 'seáis', 'estará', 'hubiera', 'fuiste', 'otras', 'estuvieras', 'hubimos', 'sentidos', 'hemos', 'fuera', 'o', 'esta', 'fuésemos', 'todos', 'muchos', 'estuviesen', 'tú', 'estás', 'nuestras', 'serás', 'sí', 'vuestros', 'en', 'estaban', 'nuestro', 'esas', 'mía', 'tuviésemos', 'sean', 'muy', 'haya', 'erais', 'entre', 'habréis', 'estaríamos', 'estarán', 'hayáis', 'los', 'habríamos', 'hube', 'míos', 'tuviste', 'para', 'tenemos', 'suyas', 'siente', 'tendrás', 'la', 'esa', 'sobre', 'hubiéramos', 'tendríamos', 'sintiendo', 'esté', 'habrían', 'me', 'están', 'tuvieses', 'tenidas', 'tuyo', 'seremos', 'qué', 'estemos', 'tenéis', 'mí', 'habías', 'está', 'nuestra', 'estuve', 'las', 'nosotras', 'tuvieran', 'hayamos', 'fueran', 'uno', 'tengamos', 'a', 'estad', 'estoy', 'les', 'mucho', 'sea', 'estaríais', 'he', 'contra', 'todo', 'ellos', 'tengas', 'hay', 'teniendo', 'estuviera', 'estada', 'mis', 'ha', 'tened', 'antes', 'habidas', 'eso', 'se', 'hayas', 'tenida', 'tengo', 'tendrías', 'sus', 'habré', 'tendríais', 'tuvisteis', 'tienen', 'ellas', 'fueron', 'es', 'estados', 'hubo', 'suyo', 'porque', 'tuviéramos', 'estáis', 'algunos', 'sentida', 'tiene', 'tendrá', 'vosotras', 'tienes', 'el', 'estuvieran', 'te', 'suyos', 'estarías', 'habremos', 'sin', 'eres', 'será', 'al', 'otra', 'habría', 'fuéramos', 'teníais', 'fuesen', 'habrás', 'quienes', 'tenían', 'estuvisteis', 'hubiésemos', 'teníamos', 'tuvieras', 'estaba', 'serías', 'son', 'él', 'hubierais', 'y', 'tuya', 'os', 'fueses', 'más', 'estuvimos', 'estábamos', 'su', 'estéis', 'algunas'}\n"
     ]
    }
   ],
   "source": [
    "palabrasVacias = set(stopwords.words('spanish'))\n",
    "palabrasVacias.add(\"davivienda\") #Se agrega \"davivienda\" a la lista de stopwords, esto debido a que es nuestro tema principal entonces queda implicito\n",
    "                                 #dentro del mensaje\n",
    "print(palabrasVacias) #Se presenta el listado de palabras utilizado para el desarrollo del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la función que en breve utilizaremos para eliminar las stopwords de una lista, llamada **quitar_stopwords** la cual toma las palabras definidas para cada tópico y elimina palabras que no pooseen relevancia dentro del contexto textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBkTvMDn1L9g"
   },
   "outputs": [],
   "source": [
    "def quitar_stopwords(lista):\n",
    "  aux_lista=[]\n",
    "  for label in lista:\n",
    "    if label not in palabrasVacias:\n",
    "      aux_lista.append(label)\n",
    "  return aux_lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera la función **remover_palabras_cortas** que elimina de una lista de palabras aquellas palabras con menos de 3 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_palabras_cortas(tweet_token):\n",
    "  tokens_aux=[]\n",
    "  for tok in tweet_token:\n",
    "    if len(tok[0]) > 3:\n",
    "        tokens_aux.append(tok)\n",
    "  return tokens_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la función **limpiar_tweets** que realiza algunos procesos sobre los tweets:\n",
    "* Concatena texto del tweet con su emoji (si es que éste existe).\n",
    "* Elimina caracteres especiales, dígitos (incluyendo el @ tan común en redes sociales) y saltos de linea.\n",
    "* Deja en minuscula todo el texto\n",
    "\n",
    "Esta función recibe un DataFrame en pandas y devuelve este mismo después de realizar todasd las modificaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcLySTZva2v-"
   },
   "outputs": [],
   "source": [
    "def limpiar_tweets(DataFrame):\n",
    "  aux_df=copy.copy(DataFrame)\n",
    "  aux_df.loc[~aux_df[\"Emojis\"].isna(),\"Embedded_text\"]=aux_df.loc[~aux_df[\"Emojis\"].isna(),\"Embedded_text\"]+\" \"+aux_df.loc[~aux_df[\"Emojis\"].isna(),\"Emojis\"]\n",
    "  aux_df[\"Embedded_text\"]=aux_df[\"Embedded_text\"].apply(lambda x:re.sub(r'[()]','',re.sub(r'\\d+', '',x)).replace(\"\\n\",\" \").replace(\"@\",\"\"))\n",
    "  aux_df[\"Embedded_text\"]=aux_df[\"Embedded_text\"].apply(lambda x:x.lower())  \n",
    "  return aux_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76ThpYsia2zR"
   },
   "outputs": [],
   "source": [
    "tweets=limpiar_tweets(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding y aplicación del modelo BERTopics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proceso de Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga el modelo de transformación de sentencias con una base previamente entrenada en español (Que corresponde al idioma de los tweets)\n",
    "sentence_model = SentenceTransformer(\"hiiamsid/sentence_similarity_spanish_es\")\n",
    "\n",
    "# Realiza el proceso de embedding para cada uno de los tweets\n",
    "embeddings = sentence_model.encode(tweets[\"Embedded_text\"], show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generación del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define el modelo UMAP  para reducir la dimensionalidad de los datos luego del embedding, éste utiliza un número de vecinos cercanos de 15,\n",
    "#10 componentes y utiliza como métrica a coseno\n",
    "umap_model = umap.UMAP(n_neighbors=15,\n",
    "                       n_components=10,\n",
    "                       min_dist=0.0,\n",
    "                       metric='cosine',\n",
    "                       low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define el modelo HBDSCAN para generar los clisters, se definen además los parametros a utilizar como un mínimo de 10 tweets para cada cluster,\n",
    "#al menos una muestra paras obtener los grupos, se utiliza la metrica euclidiana para clasificar en cada cluster, como método de selección de cluster\n",
    "# se utiliza 'eom' o  Excess of Mass algorithm  (Algoritmo de exceso de masa)\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10,\n",
    "                                min_samples=1,\n",
    "                                metric='euclidean',\n",
    "                                cluster_selection_method='eom',\n",
    "                                prediction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-wl6XObuZaA",
    "outputId": "73119f7c-00a1-4a40-d055-fddf99ad057e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 05:54:32,084 - BERTopic - Reduced dimensionality\n",
      "2022-08-09 05:54:32,522 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# Con base en los métodos de cluster y reducción de la dimensionalidad (HBDSCAN y UMAP respectivamente) se genera un primer modelo de BERTopics, \n",
    "# el cual brinda un conjunto de expresiones para la definición de cada tópico\n",
    "topic_model = BERTopic(top_n_words=20,\n",
    "                       n_gram_range=(1,2), \n",
    "                       calculate_probabilities=True,\n",
    "                       umap_model= umap_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       verbose=True)\n",
    "\n",
    "# Se realiza el entrenamiento y de este se extraen los tópicos y un vector de probabilidades de pertenencia a los diferentes tópicos  para cada tweet.\n",
    "topics, probabilities = topic_model.fit_transform(tweets[\"Embedded_text\"], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepocesamiento posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentos relevantes por tópico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOvF676Nz0z0"
   },
   "outputs": [],
   "source": [
    "def get_most_relevant_documents(cluster_id, condensed_tree):\n",
    "          \n",
    "    assert cluster_id > -1, \"La categoria del tweet debe ser mayor a -1!\"\n",
    "        \n",
    "    raw_tree = condensed_tree._raw_tree\n",
    "    \n",
    "    # Se excluyen puntos únicos y sólo se tienen en cuenta los elementos del arbol\n",
    "    cluster_tree = raw_tree[raw_tree['child_size'] > 1]\n",
    "    \n",
    "    # Se obtienen los nodos para cada rama del arbol que se está considerando\n",
    "    leaves = hdbscan.plots._recurse_leaf_dfs(cluster_tree, cluster_id)\n",
    "    \n",
    "    # Se toman los puntos maás relevantes de cada rama en el arbol\n",
    "    result = np.array([])\n",
    "    \n",
    "    for leaf in leaves:\n",
    "        max_lambda = raw_tree['lambda_val'][raw_tree['parent'] == leaf].max()\n",
    "        points = raw_tree['child'][(raw_tree['parent'] == leaf) & (raw_tree['lambda_val'] == max_lambda)]\n",
    "        result = np.hstack((result, points))\n",
    "        \n",
    "    return result.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyJoDq05z5wY"
   },
   "outputs": [],
   "source": [
    "# Se obtiene el modelo de cluster, el arbol de clusters y los ID's de tema para cada cluster.\n",
    "clusterer = topic_model.hdbscan_model\n",
    "tree = clusterer.condensed_tree_\n",
    "clusters = tree._select_clusters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2daCc8usMbeF",
    "outputId": "48ebbda1-ea46-4909-ccf5-ec1f309cea7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "#Se genera una lista de los tweets más relevantes para cada uno de los tópicos generados\n",
    "lista_elementos=pd.concat([pd.DataFrame(get_most_relevant_documents(clusters[i], tree)) for i in tweets_2.Topicos.unique()]).drop_duplicates()[0].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace uso de esta lista para filtrar unicamente los tweets más relevantes dentro del conjunto de datos inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNlY7BPm4f9r"
   },
   "outputs": [],
   "source": [
    "# Se genera un DataFrame con la categorización de cada tweet dentro de un tópcio\n",
    "tweets_2=pd.concat([tweets,pd.DataFrame(topics).rename(columns={0:\"Topicos\"})],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnPMP9n6NoLS"
   },
   "outputs": [],
   "source": [
    "# Con este último DataFrame  tenemos la certeza de solo poseer los tweets más relevantes para cada tópico\n",
    "tweets_2=tweets_2.iloc[lista_elementos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretación de Resultados y conclusiones generales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expresiones relevantes por tópico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un primer vistazo a los tópicos generados por el modelo BERTopcis y se aplica sobre el conjunto de datos las funciones **quitar_stopwords** y **remover_palabras_cortas**, lo cual nos asegura ver una lista de tópicos más limpia para cada tópico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más adelante se procede a detallar y realizar un análisis de cada uno de los tópicos (con los resultados de este ejercicio y el que posteriormente se obtendrá con otras metodologías)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdcVwcwC1eHO",
    "outputId": "1f41e078-fe9e-4a5c-a357-3a30f36c12a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópico -1 [('hola', 0.03808600200264107), ('respuesta', 0.028080388732386427), ('para', 0.028036878823205113), ('en respuesta', 0.026623189751053003), ('lo sucedido', 0.026389393864938035), ('sucedido', 0.025980512765610125), ('por favor', 0.02462442362867021), ('favor', 0.02441723195137361), ('mensaje', 0.02284908601983506), ('atencin', 0.02241858415940719), ('por mensaje', 0.020979216368972115), ('respondiendo', 0.019945743087047434)]\n",
      "Tópico 0 [('davivienda', 0.03597118183250387), ('respuesta', 0.021158645019804568), ('en respuesta', 0.020340724182409488), ('para', 0.011725035896674204)]\n",
      "Tópico 1 [('gusto lo', 0.07495216588701918), ('con gusto', 0.07350142615869872), ('gusto', 0.07332892834065875), ('privado con', 0.06867022108797423), ('lo validaremos', 0.06849087188197159), ('validaremos', 0.06849087188197159), ('validaremos quedamos', 0.06849087188197159), ('favor escrbanos', 0.06834040166353986), ('escrbanos', 0.06834040166353986), ('escrbanos por', 0.06834040166353986), ('lamentamos los', 0.06526474321920764), ('los inconvenientes', 0.06526474321920764), ('inconvenientes presentados', 0.06447949191003616), ('presentados', 0.06447949191003616), ('por favor', 0.062488034238905525), ('detalles de', 0.062470050539324806), ('por mensaje', 0.06211075958412678), ('favor', 0.06196225540971693)]\n",
      "Tópico 2 [('pedimos detallar', 0.08694402039954646), ('detallar', 0.08629077713069812), ('le pedimos', 0.08629077713069812), ('detallar su', 0.08345499329393412), ('pedimos', 0.08238799249653461), ('para', 0.07918025966092858), ('continuamente', 0.07891903010084354), ('continuamente para', 0.07891903010084354), ('trabajamos continuamente', 0.07891903010084354), ('trabajamos', 0.07745144135931066), ('para mejorar', 0.07745144135931066), ('podamos', 0.07479815728252755), ('mejorar', 0.07460666364460793), ('experiencia', 0.07404344937608393), ('privado para', 0.06802506389253386), ('para que', 0.06182792922180582), ('quedamos atentos', 0.06067758433642528), ('quedamos', 0.06019016700382351)]\n",
      "Tópico 3 [('caso quedamos', 0.1199417740199232), ('escribanos', 0.114388638304567), ('favor escribanos', 0.114388638304567), ('validar', 0.11374654866041435), ('validar su', 0.11228010931984393), ('para validar', 0.11228010931984393), ('inconvenientes por', 0.1118822104956752), ('de inconvenientes', 0.11034643209832935), ('lamentamos este', 0.11034643209832935), ('este tipo', 0.10577453256589978), ('tipo de', 0.10410701693247118), ('tipo', 0.10161586358963169), ('privado para', 0.10043150496984975), ('escribanos por', 0.08956571456022637), ('inconvenientes', 0.08884325439640556), ('lamentamos', 0.08860653969760751), ('su caso', 0.08103100788936643), ('quedamos atentos', 0.07864795131836094), ('atentos', 0.0786036002854136)]\n",
      "Tópico 4 [('para', 0.05228628662926676), ('respuesta', 0.0458304794751818), ('feliz', 0.04518688238778838), ('mensaje', 0.04464063395187807), ('mensaje privado', 0.039945050061376344), ('privado', 0.03947288379340756), ('buenas', 0.037498333557407094), ('en respuesta', 0.037219022790866925), ('quedamos', 0.03577804728849127), ('contactarnos por', 0.035695497906676546), ('contactarnos', 0.03508766916746789), ('por mensaje', 0.03453571245684358), ('tardes', 0.034155037539805706), ('importante', 0.03375679850268742), ('tarde en', 0.03280835967694588), ('invitamos', 0.03211981757534843), ('buenas tardes', 0.031171222017978915)]\n",
      "Tópico 5 [('evidenciamos', 0.07741608060406381), ('evidenciamos que', 0.07741608060406381), ('que su', 0.07464384578462363), ('su novedad', 0.07257757556630982), ('novedad', 0.07120558587667765), ('inquietud adicional', 0.06919117099184695), ('adicional', 0.06886894334038561), ('mensaje interno', 0.06881026083770564), ('inquietud', 0.06844085590131442), ('interno', 0.06543979481164658), ('atendida por', 0.060743928607181585), ('atentos para', 0.060743928607181585), ('atendida', 0.059997612545360526), ('para ayudarle', 0.0560411451796255), ('en caso', 0.05568193455658312), ('de alguna', 0.05568193455658312), ('ayudarle', 0.05537107372209081), ('caso de', 0.053223555415293874), ('alguna inquietud', 0.052036844148240516), ('alguna', 0.05090105135729251)]\n",
      "Tópico 6 [('para', 0.1073009916300731), ('brindar', 0.07132240647943612), ('brindar acompaamiento', 0.07073508725285123), ('importante brindar', 0.07073508725285123), ('nosotros es', 0.07066203286346437), ('para nosotros', 0.07024916156052641), ('acompaamiento', 0.07018317100759991), ('importante', 0.06801555683441107), ('mejorar', 0.06775220118549101), ('acompaamiento su', 0.06742753797390805), ('experiencia', 0.06724073204094717), ('nosotros', 0.06673108998944134), ('para mejorar', 0.06642808167567005), ('trabajamos', 0.06642808167567005), ('experiencia para', 0.06610457600336306), ('favor contctenos', 0.06556076286157558), ('contctenos', 0.06528453217859453), ('mejorar su', 0.06434543926778241)]\n",
      "Tópico 7 [('mensaje', 0.05246145620537275), ('hola', 0.0467589906416857), ('para', 0.040964393398471494), ('asistencia', 0.035344164332984006), ('mensaje privado', 0.034425034470356435), ('en respuesta', 0.03423100289864792), ('privado', 0.03401811696679617), ('respuesta', 0.033430193928871726), ('atentos', 0.03133341194911911), ('escribirnos', 0.028584340938439208), ('por mensaje', 0.02809813952973564), ('interno', 0.02734935049857953), ('invitamos', 0.026962181151404484), ('gracias por', 0.026452494319609936)]\n",
      "Tópico 8 [('para revisar', 0.1026707988224515), ('revisar su', 0.1026707988224515), ('nombre nmero', 0.09866704874570398), ('nicamente por', 0.09866704874570398), ('envenos su', 0.09866704874570398), ('documento nicamente', 0.09866704874570398), ('de documento', 0.09779452304303252), ('revisar', 0.09692525197329721), ('nicamente', 0.09617969612490743), ('su nombre', 0.09542924625884705), ('documento', 0.09471225028277658), ('envenos', 0.09097492410192386), ('favor envenos', 0.09097492410192386), ('nombre', 0.08749158354292608), ('nmero de', 0.0862029191326941), ('nmero', 0.08096931537795295), ('caso por', 0.071373223750595), ('su caso', 0.07002127627509286)]\n",
      "Tópico 9 [('ocasionadas', 0.08021973167485773), ('molestias ocasionadas', 0.08021973167485773), ('antemano', 0.0797092643379776), ('las molestias', 0.0797092643379776), ('de antemano', 0.0797092643379776), ('disculpas por', 0.0797092643379776), ('molestias', 0.0797092643379776), ('disculpas', 0.07921677318237115), ('por las', 0.07874103910634596), ('ofrecemos disculpas', 0.07599955987608721), ('ofrecemos', 0.07599955987608721), ('ocasionadas para', 0.07599955987608721), ('solicitud por', 0.070782116183698), ('favor contctenos', 0.0687590826444323), ('contctenos', 0.06846937630895045), ('antemano ofrecemos', 0.06742514252341941), ('con su', 0.06655202857953725), ('su solicitud', 0.0662307627139199)]\n",
      "Tópico 10 [('amplenos la', 0.10597043538904764), ('amplenos', 0.10064597546357633), ('favor amplenos', 0.10064597546357633), ('informacin de', 0.09757494430405), ('antemano ofrecemos', 0.09212377974390676), ('la informacin', 0.09212377974390676), ('ofrecemos', 0.09085922025597987), ('ocasionadas para', 0.09085922025597987), ('ofrecemos disculpas', 0.09085922025597987), ('molestias ocasionadas', 0.0902630895232155), ('ocasionadas', 0.0902630895232155), ('las molestias', 0.08968871264653581), ('molestias', 0.08968871264653581), ('disculpas por', 0.08968871264653581), ('de antemano', 0.08968871264653581), ('antemano', 0.08968871264653581), ('disculpas', 0.08913456253483916), ('privado quedamos', 0.08913456253483916)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in topic_model.topics:\n",
    "  print(f\"Tópico {i}\",quitar_stopwords(remover_palabras_cortas(topic_model.topics[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resumenes de ejemplo por tópico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo el proceso construido hasta ahora es útil para la identificación subjetiva de tópicos en el conjunto de datos que contiene los tweets, sin embargo se busca ir más allá y generar información mucho más accesible para el usuario final. Para este caso se busca generar un resumen por tópico según se hayan generado la clasificación de tweet según su tematica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bajo esta idea, se hace uso del transformador tipo Encoder-Decoder'mrm8488/bert2bert_shared-spanish-finetuned-summarization' de la plataforma HugginFace que permite realizar resumenes de textos y nos brinda una idea amplia del tipo de información con la cual nos podremos topar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212,
     "referenced_widgets": [
      "412df2b0bada47298e100f46693e0ae5",
      "ca4d930e59ee4ca9a65aa12a8d633edb",
      "bbfe7b5c5f1248ec87425902a65cb575",
      "6cba1b34a815452d8a59c328b18b52ea",
      "3947c37adda54f8d909b7d4b0ea57a75",
      "054e01fb687a4e399ff4c18e0e55c2b4",
      "d6fdc670519b48128f294b2671da6ff2",
      "4e10e8c1325e4c448fd4d1436dcf373f",
      "e3d679029e0645c89896e5936215a175",
      "764cbdfde84949689c49cf9f473aecb5",
      "50e15afaf2db4eff9fb1de7e175a4c68",
      "21a83b43d3554f13ae238a8975cc7ac2",
      "3d21eba710934998b3f93ece8c81927b",
      "d20784c4433942ef82fe300570bf73e6",
      "714c690095d040268c0f7fb0d9181b97",
      "58bafdcac9964bd188fbd7573d948601",
      "42c3ec13acd44ce294ec1b6d0521fbb6",
      "808436d208b2410da2d5bac6f92df74b",
      "77065a9e09e04ef4a9bcfcf882a42ec5",
      "1fec3e3dea124f6281860d523a836952",
      "1d22fc75e2b948d39215f51be0e651fd",
      "4956d843446b4af2beb8b68aaab0bb56",
      "eaff1f302541447cb4cb62ec039c02e5",
      "67ae51ff17124e6682fa5e960c60463b",
      "d3cdf6acdab6411fa0329bfd399e946b",
      "a31a17e2153a4806954b1f83d287d1e7",
      "dc0d53fbb6964e45808450494b492471",
      "be8e448c35bd4d4d9b62c8c87c3d0dba",
      "12f36d7936c948abb33727f224afe925",
      "f3203e049a6f4b62b4c97255af8506d4",
      "f4a7a7b065ca49a3a78cbd75ca8a34b4",
      "d8682167afdd4e7caa8c2e4de702dc17",
      "929ab28cf42a428a9d58d20014bddf66",
      "873d22b1520e48bf98de7d651ccddb89",
      "2518d1420b9d49adaa3a1cf51dc6cc90",
      "8e79e1677ca34636bb38a93565bb271c",
      "ce201955d2864df8ab5899da116fddc5",
      "1f894d16c899423fafcead0f58754c9c",
      "45e62deb8fda4bbe9cd4f64bd98af03b",
      "55480c2130974d16b8562703a41fbb37",
      "e47f97beecf84222b7d88458406e4c6f",
      "d7361db406334bd69636c8e88bc2d635",
      "22cfb6a092ce4dfbb6e327e986cfcf22",
      "35803f1bd432486a89df1c6466886ddb",
      "da110a6f59bd4103b47f71e9111a2943",
      "3f9c80af034a4618be15fdc9f3ffdb6a",
      "98def0362376493a8901d8ce537ba553",
      "473063338082472490d7c7159c1a7ba9",
      "c155d3fadcaf488ba7a511f20a58e342",
      "14438b97138e4c81a160e59e0c599f16",
      "4bdae802353644578d1318183f51d569",
      "164cdae9d8d44167b32cdff9cc568623",
      "a2ee5b8a6ac84042a0815159d998038f",
      "adfc09ecf59448e3a1fcb2f646afcbcd",
      "84dc69f368b34495a37fd1e425aff583"
     ]
    },
    "id": "TFIbE4_w6frH",
    "outputId": "87a44752-e5c4-4ae0-ba64-e783907ce514"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412df2b0bada47298e100f46693e0ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/520 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a83b43d3554f13ae238a8975cc7ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/236k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaff1f302541447cb4cb62ec039c02e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873d22b1520e48bf98de7d651ccddb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/4.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da110a6f59bd4103b47f71e9111a2943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/530M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Esta linea hace referencia a la activación de CUDA de Nvidia para poder realizar el proceso \n",
    "                                                        # que requiere el uso de la GPU de Colab\n",
    "ckpt = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization' # Se nombra el nombre del transformer a utilizar, esta elección se realizó dado\n",
    "                                                                  # que es uno de los transformadores bert2bert mejor puntuados en HugginFace\n",
    "tokenizer = BertTokenizerFast.from_pretrained(ckpt) #Se carga el tokenizador pre-entrenado con el modelo previamente definido\n",
    "model_3 = EncoderDecoderModel.from_pretrained(ckpt).to(device) #Se carga el modelo de resumen que utilizaremos\n",
    "\n",
    "#Se genera la función que regala un breve resumen  para un texto (que en este caso se usará la concatenación de tweets por tópico)\n",
    "\n",
    "def generate_summary(text):\n",
    "\n",
    "   inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "   input_ids = inputs.input_ids.to(device)\n",
    "   attention_mask = inputs.attention_mask.to(device)\n",
    "   output = model_3.generate(input_ids, attention_mask=attention_mask)\n",
    "   return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3kLUg9z6ywk"
   },
   "outputs": [],
   "source": [
    "# Se genera una función que recibe un DataFrame y devuelve un diccionario para el tópico que se le solicite\n",
    "def resumen_topico(DataFrame,campo_texto,campo_topico,topico_nom):\n",
    "  df_aux=copy.copy(DataFrame)\n",
    "  df_aux=df_aux.query(f\"{campo_topico}=={topico_nom}\")\n",
    "  combined=\"\"\n",
    "  for i in df_aux[campo_texto]:\n",
    "    combined+=f\"{i}. \"\n",
    "  return {'topico':topico_nom,'texto':generate_summary(combined)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QU38nbxP76n_"
   },
   "outputs": [],
   "source": [
    "resmune_topicos=[resumen_topico(tweets_2,'Embedded_text','Topicos',topico) for topico in tweets_2[\"Topicos\"].unique()] #Se genera el proceso resumir cada \n",
    "                                                                                                                       #tópico por sus tweets más relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "extmcPZj4URt",
    "outputId": "458cc5c5-b50a-4259-88cc-7a8fa34670af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'texto': 'El concierto davivienda y la wellagency de sfcsupervisor en el décimo aniversario de la celebración navideña',\n",
       "  'topico': 0},\n",
       " {'texto': 'En respuesta a las molestias ocasionadas, ofrecemos una selección de artículos de EL PAÍS',\n",
       "  'topico': 10},\n",
       " {'texto': 'En respuesta a alfredmaggiore para revisar su caso por favor envíenos su nombre y número de documento, únicamente por mensaje privado',\n",
       "  'topico': 8},\n",
       " {'texto': 'En respuesta a las urnas sra. erikasu, sra erikauu / buenas tardes, sr. camilo',\n",
       "  'topico': 6},\n",
       " {'texto': 'En respuesta a la pregunta de lsssshhhh buenas tardes. lamentamos los inconvenientes presentados.',\n",
       "  'topico': 1},\n",
       " {'texto': 'En respuesta a marianiniecheve hola maria, gracias por escribirnos, nos interesa conocer los detalles de su caso y confírmenos sus datos por mensaje interno',\n",
       "  'topico': 7},\n",
       " {'texto': 'Ha sido atendida por mensaje interno y nos encontramos atentos para ayudar en caso de alguna inquietud adicional',\n",
       "  'topico': 5},\n",
       " {'texto': '\" Por favor escribanos por mensaje privado para validar su caso... \"',\n",
       "  'topico': 3},\n",
       " {'texto': 'En respuesta a las molestias ocasionadas por las molestias de las vacaciones, nuestro sistema de ayuda ayuda con su petición de ayuda',\n",
       "  'topico': 9},\n",
       " {'texto': 'El periodista de EL PAÍS Rubén Amón analiza los horarios de nuestras oficinas a nivel nacional disponible en esta emergencia',\n",
       "  'topico': 4},\n",
       " {'texto': 'En respuesta a misislatierna buenos días. Trabajar continuamente para mejorar su experiencia. Le pedimos detallar su caso por mensaje privado',\n",
       "  'topico': 2}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resmune_topicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tema de interpretación textual se puede presentar para ser algo muy subjetivo, por lo tanto se compararán ambos resultados que se obtuvieron en este proceso: las expresiones por tópico y el ejemplo de resumen por tópico. Cabe aclarar que el resumen se hace sobre todos los tweets de un mismo tópicos por lo tanto algunos de los resumenes pueden no tener sentido lógico pero pueden llegar a dar una idea de la intención de los textos. Aquí presento mi análisis subjetivos de cada uno de los tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tópico 0**: Se relaciona principalmente una respuesta brindada por la compañia davivienda, que se podría interpretar sobre cualquier tema (eso incluyendo la de cualquier evento que pueda tener cabida la participación de la compañia como podría sugerir el resumen)\n",
    "\n",
    "* **Tópico 1**: Este está más referido a la respuesta para una validación u inconvenientes presentados, como podemos inferir de las expresiones de la primera parte (es claro que el resumen podría ser un ejemplo claro de éste tema, como bien dice está relacionado a conocer más a profundidad y validar un caso \"...buenas tardes. lamentamos los inconvenientes presentados.\")\n",
    "\n",
    "* **Tópico 2**: Va claramente relacionado con la solicitud de detalles de alguna problematica presentada por el usuario, aquí cabe resaltar que se suguiere la comunicación directa del cliente y además se brindan palabras que tranquilicen al usuario, además de mostrar la disposición de la compañia a buscar la mejora constante (el resumen está claramente relacionado con las expresiones generadas por BERTopics \"Trabajar continuamente para mejorar su experiencia. Le pedimos detallar su caso por mensaje privado\")\n",
    "\n",
    "* **Tópico 3**: Aquí claramente se involucra el tema de molestias, se solicita el apoyo al usuario para diligenciar el caso y se brindan disculpas por percances en el camino (mucho mejor explicado por el ejemplo del resumen \"Por favor escribanos por mensaje privado para validar su caso... \")\n",
    "\n",
    "* **Tópico 4**: Se puede brindar como una respuesta a cualquier caso y se brindan los canales de comunicación para dicha acción\n",
    "\n",
    "* **Tópico 5**: Este tema va encaminado cuando ya se posee solución a una solicitud o duda que hayan podido tener los usuarios. Es muy contundente con palabras como \"evidencia\" (el resumen de ejemplo brinda un buen acercamiento a la idea central del tópico \"Ha sido atendida por mensaje interno y nos encontramos atentos para ayudar en caso de alguna inquietud adicional\")\n",
    "\n",
    "* **Tópico 6**: Bastante inclinado a brindar acompañamiento y generar cercania con el cliente, ya sea por una duda o una solicitud. Esta más relacionado propiamente a esa disposición por parte de la compañia al servicio amable.\n",
    "\n",
    "* **Tópico 7**: Se solicita remisión de un caso como mensaje privado y detallar dicho proceso.\n",
    "\n",
    "* **Tópico 8**: Solicitud de datos personales (Tal como se ve en el resumen \"por favor envíenos su nombre y número de documento, únicamente por mensaje privado\")\n",
    "\n",
    "* **Tópico 9**: Claramente inclinado a brindar disculpas por fallas a nivel general.\n",
    "\n",
    "* **Tópico 10**: Disculpas por inconvenientes presentados \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIMPLIFICACIÓN DE TÓPICOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según mi criterio propio y habiendo interpretado cada uno de los tópicos puedo sugerir los siguientes temas en los cuales agrupar cada conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Respuesta general o genérica de Davivienda** (Tópicos 0 y 4)\n",
    "\n",
    "* **Validación en inconvenientes** (Tópicos 1 y 3)\n",
    "\n",
    "* **Solicitud de detalles del caso en mensaje privado** (Tópicos 2 y 7)\n",
    "\n",
    "* **Ofreciendo disculpas** (Tópico 9 y 10)\n",
    "\n",
    "* **Solución hallada o evidencia encontrada sobre algún caso en particular** (Tópico 5)\n",
    "\n",
    "* **Disposición de servicio y acompañamiento al cliente ante cualquier problema o inquiertud** (Tópico 6)\n",
    "\n",
    "* **Solicitud de datos personales por mensaje interno** (Tópico 8)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Prueba_de_conocimientos_II_DNE_Davivienda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
